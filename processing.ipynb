{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- consider adding the title of the book and the author to some of the prompts where it's useful; especially where ambiguity about the purpose of the text could significantly change the stuff generated. So the judged worthy for questions prompt, some of the RP prompts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your desired quant of your desired model in the relevant directories\n",
    "RP_MODEL = \"./rp_model\" # model used for RP tasks, probably going to use Sao10K/Euryale-1.3-L2-70b\n",
    "LOGICAL_MODEL = \"./logical_model/airoboros-l2-70b-3.1.2.Q4_K_M.gguf\" # model used for decision-making and base question generation (should be \"smart\")\n",
    "RAM_IS_LIMITED = True # change this to false if you can fit both models on your GPU at once\n",
    "ASSISTANT_MODE = False # change to true if you want all conversations to be with an AI language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Gryphe/MythoMax-L2-13b\")\n",
    "\n",
    "def sentence_chunking_algorithm(file_path, tokenizer, max_token_length=400):\n",
    "    \"\"\"\n",
    "    This function takes a plaintext file and chunks it into sentences.\n",
    "    \n",
    "    :param file_path: Path to the plaintext file\n",
    "    :param tokenizer: SentencePiece tokenizer\n",
    "    :param max_token_length: The maximum token length for a chunk of sentences\n",
    "    :return: List of sentence chunks with source text information\n",
    "    \"\"\"\n",
    "    sentence_chunks_with_source = []\n",
    "    current_chunk = []\n",
    "    token_count = 0\n",
    "    source_name = file_path.replace(\".txt\",\"\")\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Remove Gutenberg header and footer\n",
    "    content = re.sub(r'^.*?START OF (THIS|THE) PROJECT GUTENBERG EBOOK.*$\\n', '', content, flags=re.MULTILINE)\n",
    "    content = re.sub(r'^.*?END OF (THIS|THE) PROJECT GUTENBERG EBOOK.*$\\n', '', content, flags=re.MULTILINE)\n",
    "\n",
    "    sentences = sent_tokenize(content)\n",
    "\n",
    "    for sentence in tqdm(sentences, desc=f\"Processing {file_path}\"):\n",
    "        sentence_token_count = len(tokenizer.encode(sentence))\n",
    "\n",
    "        if token_count + sentence_token_count <= max_token_length:\n",
    "            current_chunk.append(sentence)\n",
    "            token_count += sentence_token_count\n",
    "        else:\n",
    "            sentence_chunks_with_source.append((' '.join(current_chunk), source_name))\n",
    "            current_chunk = [sentence]\n",
    "            token_count = sentence_token_count\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        sentence_chunks_with_source.append((' '.join(current_chunk), source_name))\n",
    "\n",
    "    return sentence_chunks_with_source\n",
    "\n",
    "source_texts = [\n",
    "    \"Principles of Chemistry, by Demitry Mendeleev, published 1897.txt\",\n",
    "    \"Simple Sabotage, by the Office of Strategic Services, published 1944.txt\",\n",
    "    \"Introduction to Philosophy, by George Stuart Fullerton.txt\",\n",
    "]\n",
    "\n",
    "sentence_chunks = []\n",
    "for source_text in source_texts:\n",
    "    sentence_chunks += sentence_chunking_algorithm(source_text, tokenizer)\n",
    "\n",
    "def fix_text(to_replace_arr, text):\n",
    "    for startup in to_replace_arr:\n",
    "        text = text.replace(startup[0], startup[1])\n",
    "    return text\n",
    "\n",
    "conversions = [(\"\\n\",\" \"), (\"  \", \" \")]\n",
    "\n",
    "paragraphs_processed = [(fix_text(conversions, seq[0]), seq[1]) for seq in sentence_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paragraphs_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_processed[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_processed[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_processed[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_processed[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rp_llm = Llama(model_path=RP_MODEL,n_ctx=4096,n_gpu_layers=100) # load the RP llp and offload everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logic_llm = Llama(model_path=LOGICAL_MODEL,n_ctx=12000,rope_freq_scale=0.33,n_gpu_layers=100,verbose=False) # load the logical LLM and offload everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in no way best practices, but all my prompts being searchable and separate files is a good way to make my life easier.\n",
    "import pkgutil\n",
    "import importlib\n",
    "import generation_functions  # This is the package directory\n",
    "import sys\n",
    "\n",
    "sys.path.append('./generation_functions')\n",
    "\n",
    "# First, import all modules so they can be reloaded\n",
    "for _, module_name, _ in pkgutil.iter_modules(generation_functions.__path__, generation_functions.__name__ + '.'):\n",
    "    importlib.import_module(module_name)\n",
    "\n",
    "# Now, reload each module and import all callable attributes\n",
    "for _, module_name, _ in pkgutil.iter_modules(generation_functions.__path__, generation_functions.__name__ + '.'):\n",
    "    # Reload the module\n",
    "    module = importlib.reload(sys.modules[module_name])\n",
    "    # Iterate through each attribute in the reloaded module\n",
    "    for attribute_name in dir(module):\n",
    "        # Retrieve the attribute\n",
    "        attribute = getattr(module, attribute_name)\n",
    "        if callable(attribute):\n",
    "            # If it's callable, it's a function or class, so you set it in the globals dictionary\n",
    "            globals()[attribute_name] = attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which paragraphs are worthy of making questions from\n",
    "judged_worthy_for_questions = []\n",
    "for idx, p in tqdm(enumerate(paragraphs_processed)): # for each paragraph, try determining if it is suitable for questions or not, at most three times each\n",
    "    judgement = judge_paragraph(p,logic_llm)\n",
    "    judged_worthy_for_questions.append(judgement)\n",
    "    try:\n",
    "        if judgement[0] is not None:\n",
    "            print(f\"DEBUG model decided that index {idx} was suitable\")\n",
    "        elif judgement[0] is None:\n",
    "            print(f\"DEBUG model decided that index {idx} was not suitable\")\n",
    "    except:\n",
    "        print(f\"DEBUG max retries exceeded for index {idx}\")\n",
    "    \n",
    "        \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def filter_and_graph(tuples):\n",
    "    # Count the occurrences of None and non-None for each source text\n",
    "    source_counts = Counter()\n",
    "    for paragraph, source in tuples:\n",
    "        if paragraph is None:\n",
    "            source_counts[source] = source_counts.get(source, [0, 0])\n",
    "            source_counts[source][0] += 1\n",
    "        else:\n",
    "            source_counts[source] = source_counts.get(source, [0, 0])\n",
    "            source_counts[source][1] += 1\n",
    "\n",
    "    # Prepare data for the graph\n",
    "    labels = list(source_counts.keys())\n",
    "    none_counts = [source_counts[source][0] for source in labels]\n",
    "    non_none_counts = [source_counts[source][1] for source in labels]\n",
    "\n",
    "    # Plotting the graph\n",
    "    x = range(len(labels))\n",
    "    plt.bar(x, none_counts, width=0.4, label='Not suitable', align='center')\n",
    "    plt.bar(x, non_none_counts, width=0.4, label='Valid Paragraphs', align='edge')\n",
    "    plt.xlabel('Source Text')\n",
    "    plt.ylabel('Number of Paragraphs')\n",
    "    plt.title('Paragraphs Suitable for Questions by Source Text')\n",
    "    plt.xticks(x, labels, rotation='vertical')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Filter out tuples with None and return the new list\n",
    "    filtered_list = [t for t in tuples if t[0] is not None]\n",
    "    return filtered_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_worthy_for_questions = filter_and_graph(judged_worthy_for_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Answer generation code, begin.\n",
    "# structure: define a series of helpers, then define the control flow, exception handling, retries etc. in a for loop that iterates over the processed sequences of paragraphs at the end in another cell\n",
    "\n",
    "# Since some paragraphs can be much shorter \n",
    "# First off, question generation.\n",
    "\n",
    "# Each local LLM function essentially has 3 phases: prompt, regex to extract response, and reaction to that response.\n",
    "# However I'm not going to build an abstraction for that because I need fine control.\n",
    "\n",
    "# If any function fails to make things, it won't throw, it'll just return None.\n",
    "\n",
    "# Strengths of open source AI: hella cheap, very customizable, you can call it as much as you want\n",
    "# Downside: you need very good regexes to catch its outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting fixer, generalist prompt\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_worthy_for_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control flow helpers\n",
    "\n",
    "# Change this value to change how many times the checks must pass consecutively for a thing to be accepted\n",
    "\n",
    "# EXPERIMENTAL ADJUSTMENT TO THE PIPELINE: majority rules for accuracy checks, instead of requiring unanimous approval, since it can be quite finicky\n",
    "# CHANGE: replaced all things like `if passed_checks == DOUBLE_CHECK_COUNTER:` with if passed_checks > ceil(DOUBLE_CHECK_COUNTER/2)\n",
    "import logging\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='data_generation.log', \n",
    "                    filemode='a', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "DOUBLE_CHECK_COUNTER = 3 # Set to 1 to check only once; set to 2 to check twice; set to 3 to check thrice; set to 0 to break everything\n",
    "\n",
    "def tokenize_and_check_length(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens) > 500, tokens\n",
    "\n",
    "def vet_answer_accuracy_loop(qa_tuple,total_retries):\n",
    "    try:\n",
    "        qtuple = qa_tuple\n",
    "        print(f\"\\n\\nStarting ACCURACY loop for question: {qtuple[0]}, context: {qtuple[2]}\")\n",
    "        passed_checks = 0\n",
    "        times_checked = 0\n",
    "        dissenting_reasoning = \"\"\n",
    "        while times_checked < DOUBLE_CHECK_COUNTER:\n",
    "            print(f\"\\n\\nACCURACY CALL CHECK ANSWER: {qtuple[0]}, context: {qtuple[2]}, retries: {total_retries}, dissenting reasoning: {dissenting_reasoning}\")\n",
    "            judgement = check_answer(qtuple, logic_llm)\n",
    "            if not judgement[0]:  # if not accurate\n",
    "                dissenting_reasoning = judgement[1]\n",
    "            else:\n",
    "                passed_checks += 1\n",
    "            times_checked+=1\n",
    "            if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                break\n",
    "            failed_checks = times_checked - passed_checks\n",
    "            if failed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                break\n",
    "        \n",
    "        if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):  # if question checks passed\n",
    "            print(f\"\\n\\ANSWER ACCURACY CHECKS PASSED retries: {total_retries}\")\n",
    "            return qtuple\n",
    "        else:\n",
    "            # Generate new question and restart the loop\n",
    "            print(f\"\\n\\nACCURACY CHECKS FAILED - SENDING BACK TO QUESTION LOOP retries: {total_retries}\")\n",
    "            total_retries += 1\n",
    "            qtuple = generate_new_question(qtuple, logic_llm)\n",
    "            return vet_question_loop(qtuple,total_retries) # going to get one hell of a call stack by the end of this, but it should be fine\n",
    "    except Exception as e:\n",
    "        print(\"!!ERROR!!\")\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    return (None, None, None, qtuple[3])\n",
    "\n",
    "def vet_answer_relevance_loop(qa_tuple,total_retries):\n",
    "    try:\n",
    "        qtuple = qa_tuple\n",
    "        print(f\"\\n\\nStarting RELEVANCE loop for question: {qtuple[0]}, context: {qtuple[2]}\")\n",
    "        passed_checks = 0\n",
    "        times_checked = 0\n",
    "        dissenting_reasoning = \"\"\n",
    "        while times_checked < DOUBLE_CHECK_COUNTER:\n",
    "            print(f\"\\n\\nRELEVANCE CALL CHECK ANSWER: {qtuple[0]}, context: {qtuple[2]}, retries: {total_retries}, dissenting reasoning: {dissenting_reasoning}\")\n",
    "            judgement = check_answer_relevancy_with_text(qtuple, logic_llm)\n",
    "            if not judgement[0]:  # if not relevant\n",
    "                dissenting_reasoning = judgement[1]\n",
    "            else:\n",
    "                passed_checks += 1\n",
    "            times_checked += 1\n",
    "            if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                break\n",
    "            failed_checks = times_checked - passed_checks\n",
    "            if failed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                break\n",
    "        \n",
    "        if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "            print(f\"\\n\\nRELEVANCE CHECKS PASSED\")\n",
    "            return vet_answer_accuracy_loop(qtuple,total_retries)\n",
    "        else:\n",
    "            print(f\"\\n\\nRELEVANCE CHECKS FAILED - SENDING BACK TO QUESTION LOOP\")\n",
    "            total_retries += 1\n",
    "            qtuple = generate_new_question(qtuple, logic_llm)\n",
    "            return vet_question_loop(qtuple,total_retries)\n",
    "    except Exception as e:\n",
    "        print(\"!!ERROR!!\")\n",
    "        print(e)\n",
    "        pass\n",
    "    \n",
    "    return (None, None, None, qtuple[3])\n",
    "\n",
    "def vet_question_loop(qa_tuple, total_retries=0):\n",
    "    try:\n",
    "        qtuple = qa_tuple\n",
    "        print(f\"\\n\\nStarting QUESTION loop for question: {qtuple[0]}, context: {qtuple[2]}\")\n",
    "        while total_retries <= 4:\n",
    "            passed_checks = 0\n",
    "            times_checked = 0\n",
    "            dissenting_reasoning = \"\"\n",
    "            while times_checked < DOUBLE_CHECK_COUNTER:\n",
    "                print(f\"\\n\\nQUESTION CALL CHECK ANSWER: {qtuple[0]}, context: {qtuple[2]}, retries: {total_retries}, dissenting reasoning: {dissenting_reasoning}\")\n",
    "                judgement = check_question(qtuple, logic_llm)\n",
    "                if not judgement[0]:  # if not relevant\n",
    "                    dissenting_reasoning = judgement[1]\n",
    "                else:\n",
    "                    passed_checks += 1\n",
    "                times_checked += 1\n",
    "                if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                    break\n",
    "                failed_checks = times_checked - passed_checks\n",
    "                if failed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                    break\n",
    "            \n",
    "            if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):  # if all question checks passed\n",
    "                print(f\"\\n\\nQUESTION CHECKS PASSED retries: {total_retries}\")\n",
    "                return vet_answer_relevance_loop(qtuple,total_retries)\n",
    "            else:\n",
    "                # Generate new question and restart the loop\n",
    "                print(f\"\\n\\nQUESTION CHECKS FAILED - GENERATING NEW QUESTION retries: {total_retries}\")\n",
    "                total_retries += 1\n",
    "                if (total_retries <= 4): # don't regen question if we're already at max regens\n",
    "                    qtuple = generate_new_question(qtuple, logic_llm) \n",
    "                    print(\"New question: \", qtuple)\n",
    "                # no calling of vet_question_loop, since we're already in a while loop\n",
    "    except Exception as e:\n",
    "        print(\"!!ERROR!!\")\n",
    "        print(e)\n",
    "    \n",
    "    return (None, None, None, qtuple[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control flow\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Directory for QA tuples\n",
    "qa_tuples_dir = './qatuples_raw'\n",
    "if not os.path.exists(qa_tuples_dir):\n",
    "    os.makedirs(qa_tuples_dir)\n",
    "\n",
    "\n",
    "# Generate questions CoT-style\n",
    "vetted_qa_tuples = [] # tuple list of qa tuples that have been judged good\n",
    "for idx, para in enumerate(tqdm(filtered_worthy_for_questions)):\n",
    "    try:\n",
    "        print(f\"\\n\\n\\nOUTER LOOP CALL GENERATE QPLAN para: {para}, \\n\\n idx: {idx}\")\n",
    "        plan = generate_questions_plan(para,logic_llm)\n",
    "        print(f\"\\n\\n\\nOUTER LOOP CALL GENERATE Q: {para}, \\n\\n idx: {idx} \\n\\n plan: {plan}\")\n",
    "        question_answer_tuples = generate_questions(para,plan,logic_llm)\n",
    "        for qnum, question_answer_tuple in enumerate(question_answer_tuples):\n",
    "            print(f\"\\n\\n=======!!=BEGIN VETTING QA TUPLE {idx}_{qnum}=!!=======\\n\\n\")\n",
    "            good_qa_tuple = vet_question_loop(question_answer_tuple)\n",
    "            \n",
    "            # Write to file if the tuple is not None\n",
    "            if good_qa_tuple[0] is not None:\n",
    "                file_path = os.path.join(qa_tuples_dir, f'para_{idx}_q_{qnum}.json')\n",
    "                with open(file_path, 'w') as file:\n",
    "                    json.dump(good_qa_tuple, file, indent=4)\n",
    "            \n",
    "            vetted_qa_tuples.append(good_qa_tuple) # We must filter out all None values at the end; but appending Nones lets us know where things went wrong, and how often.\n",
    "    except Exception as e:\n",
    "        print(f\"Q ERROR: {e}\")\n",
    "        \n",
    "print(\"-------------- QUESTIONS CREATED ------------- STATS SO FAR:\")\n",
    "nones= list(filter(lambda x: x[0] is None, vetted_qa_tuples))\n",
    "print(f\"Nones: {len(nones)}\")\n",
    "print(f\"Non-nones: {len(vetted_qa_tuples) - len(nones)}\")\n",
    "print(f\"Total: {len(vetted_qa_tuples)}\")\n",
    "# filter out all None values\n",
    "vetted_qa_tuples = [qa for qa in vetted_qa_tuples if qa[0] is not None]\n",
    "print(\"---------------- ONTO EXAMPLES GENERATION-------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to read a single JSON file\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Directory for QA tuples\n",
    "qa_tuples_dir = './qatuples_raw'\n",
    "if not os.path.exists(qa_tuples_dir):\n",
    "    os.makedirs(qa_tuples_dir)\n",
    "\n",
    "vetted_qa_tuples = [] # List of qa tuples that have been judged good\n",
    "for idx, para in enumerate(tqdm(filtered_worthy_for_questions)):\n",
    "    para_qa_tuples = []\n",
    "    for qnum in range(5): # Assuming a maximum of 5 questions per paragraph\n",
    "        file_path = os.path.join(qa_tuples_dir, f'para_{idx}_q_{qnum}.json')\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Read from existing file\n",
    "                good_qa_tuple = read_json_file(file_path)\n",
    "                para_qa_tuples.append(good_qa_tuple)\n",
    "            except Exception as e:\n",
    "                print(f\"File read error for {file_path}: {e}\")\n",
    "        else:\n",
    "            try:\n",
    "                # Generate new QA tuple\n",
    "                plan = generate_questions_plan(para, logic_llm)\n",
    "                question_answer_tuples = generate_questions(para, plan, logic_llm)\n",
    "                good_qa_tuple = vet_question_loop(question_answer_tuples[qnum])\n",
    "                \n",
    "                if good_qa_tuple[0] is not None:\n",
    "                    with open(file_path, 'w') as file:\n",
    "                        json.dump(good_qa_tuple, file, indent=4)\n",
    "                \n",
    "                para_qa_tuples.append(good_qa_tuple)\n",
    "            except Exception as e:\n",
    "                print(f\"Generation error for paragraph {idx}, question {qnum}: {e}\")\n",
    "    \n",
    "    vetted_qa_tuples.extend(para_qa_tuples)\n",
    "\n",
    "# Filter out None values\n",
    "vetted_qa_tuples = [qa for qa in vetted_qa_tuples if qa[0] is not None]\n",
    "\n",
    "# Print statistics\n",
    "nones = list(filter(lambda x: x[0] is None, vetted_qa_tuples))\n",
    "print(f\"Nones: {len(nones)}\")\n",
    "print(f\"Non-nones: {len(vetted_qa_tuples) - len(nones)}\")\n",
    "print(f\"Total: {len(vetted_qa_tuples)}\")\n",
    "```\n",
    "\n",
    "^ Temp storage for GPT-written combined function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Counting nones vs non-nones for questions per source text\n",
    "qa_counts = {}\n",
    "for qa in vetted_qa_tuples:\n",
    "    source_text = qa[3] if qa[3] is not None else 'Unknown'\n",
    "    if source_text not in qa_counts:\n",
    "        qa_counts[source_text] = {'None': 0, 'Valid': 0}\n",
    "    \n",
    "    if qa[0] is None:\n",
    "        qa_counts[source_text]['None'] += 1\n",
    "    else:\n",
    "        qa_counts[source_text]['Valid'] += 1\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Data for plotting\n",
    "sources = list(qa_counts.keys())\n",
    "none_counts = [qa_counts[src]['None'] for src in sources]\n",
    "valid_counts = [qa_counts[src]['Valid'] for src in sources]\n",
    "\n",
    "# Creating the bar plot\n",
    "ax.bar(sources, valid_counts, label='Valid QA Tuples')\n",
    "ax.bar(sources, none_counts, bottom=valid_counts, label='None QA Tuples')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('Source Text')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Number of None vs Valid QA Tuples per Source Text')\n",
    "ax.legend()\n",
    "\n",
    "# Rotate labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logic_llm = Llama(model_path=LOGICAL_MODEL,n_ctx=4096,n_gpu_layers=100,verbose=False) # load the logical LLM and offload everything\n",
    "\n",
    "\n",
    "# if RAM_IS_LIMITED:\n",
    "#     del logic_llm # goodbye sense, now it's time to get funky\n",
    "# well, at least more fluent and conversational\n",
    "\n",
    "# Not implemented yet. Most systems can probably only run one 70b anyway.\n",
    "# if not RAM_IS_LIMITED:\n",
    "#     rp_llm = Llama(model_path=RP_MODEL,n_ctx=4096,n_gpu_layers=100,verbose=False) # load the RP LLM and offload everything else\n",
    "# else:\n",
    "#     rp_llm = logic_llm # Quick and dirty way to avoid having to rename things or decide things everywhere. Just use the logic llm for both. (ASSUMPTION: this does not clone the logic llm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for generating examples\n",
    "def make_character(qa_tuple):\n",
    "    # generating the character first will allow for more creativity regarding what kinds of scenarios (and characters!) can be made. Doing the scenario first will probably just lead to nothing but casual chats between an expert and a student, which is fine, but not as interesting.\n",
    "    plan = create_character_card_plan(qa_tuple,logic_llm)\n",
    "    char = create_character_card(qa_tuple,plan,logic_llm) # creates a character card\n",
    "    return char\n",
    "\n",
    "def make_scenario(qa_tuple,character):\n",
    "    plan = create_scenario_plan(qa_tuple,character,logic_llm)\n",
    "    scenario = create_scenario(qa_tuple,character,plan,logic_llm) # creates a scenario based on a character card and question/answer tuple\n",
    "    return scenario # NEED TO FEED THIS THROUGH EDITING AI\n",
    "\n",
    "def make_single_turn_conversation(qa_tuple):\n",
    "    plan = create_character_card_plan(qa_tuple,logic_llm)\n",
    "    char = create_character_card(qa_tuple,plan,logic_llm)\n",
    "    plan_scenario = create_scenario_plan(qa_tuple,char,logic_llm)\n",
    "    scenario = create_scenario(qa_tuple,char,plan_scenario,logic_llm)\n",
    "    thought_plan = create_thought_plan(qa_tuple,char,logic_llm)\n",
    "    return (single_turn_conversation(qa_tuple,char,scenario,thought_plan,plan_scenario,logic_llm),scenario,char,qa_tuple)\n",
    "\n",
    "def ensure_answer_is_same(qatuple,conv,llm):\n",
    "    \"\"\"A loop, like the check accuracy ones for the qatuple generation, that ensures that the answer is the same in the conversation as it is in the tuple.\"\"\"\n",
    "    retries = 0\n",
    "    c = conv # conv is 4-tuple with conversation, scenario, charcard, and scenario plan\n",
    "    while retries <= 4:\n",
    "        consistence_checks = 0\n",
    "        passed_checks = 0\n",
    "        while consistence_checks < DOUBLE_CHECK_COUNTER:\n",
    "            judgement = ensure_answer_consistent(qatuple,conv[0], llm)\n",
    "            if judgement[0]:  # if consistent\n",
    "                passed_checks += 1\n",
    "            consistence_checks += 1\n",
    "            if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                    break\n",
    "            failed_checks = consistence_checks - passed_checks\n",
    "            if failed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):\n",
    "                    break\n",
    "            \n",
    "        if passed_checks >= ceil(DOUBLE_CHECK_COUNTER/2):  # if all question checks passed\n",
    "            print(f\"\\n\\CONSISTENCE CHECKS PASSED retries: {retries}\")\n",
    "            return c\n",
    "        else:\n",
    "            # Generate new question and restart the loop\n",
    "            print(f\"\\n\\CONSISTENCE CHECKS FAILED - GENERATING NEW QUESTION retries: {retries}\")\n",
    "            # If we're here, question relevance check failed\n",
    "            # Do the entire process again, since if one thing's screwed the rest will be too\n",
    "            if retries != 4:\n",
    "                retry = make_single_turn_conversation(qatuple)\n",
    "                if retry is not None:\n",
    "                    c = retry\n",
    "                else:\n",
    "                    # if we failed to generate a retry after it got broken the first time, this is just f*cked and it's a bad idea to waste compute on it\n",
    "                    return None\n",
    "            retries += 1 \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE\n",
    "# Reads qatuples from files (useful if you had to restart the notebook)\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def read_json_files(directory):\n",
    "    # List to hold all tuples\n",
    "    all_tuples = []\n",
    "    \n",
    "    # Get all the json files from the given directory\n",
    "    json_files = [f for f in Path(directory).iterdir() if f.suffix == '.json']\n",
    "    \n",
    "    # Sort files by their para_num and q_num\n",
    "    sorted_files = sorted(json_files, key=lambda x: (int(x.stem.split('_')[1]), int(x.stem.split('_')[3])))\n",
    "\n",
    "    # Read each file and extract tuples\n",
    "    for file in sorted_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # Extract the para_num\n",
    "            para_num = int(file.stem.split('_')[1])\n",
    "            \n",
    "            # Ensure we have the list to hold tuples for this para_num\n",
    "            if para_num >= len(all_tuples):\n",
    "                all_tuples.extend([[] for _ in range(para_num - len(all_tuples) + 1)])\n",
    "            \n",
    "            # Extract the tuple and add to the corresponding sublist\n",
    "            q_a_tuple = (data[0], data[1], data[2], data[3])\n",
    "            all_tuples[para_num].append(q_a_tuple)\n",
    "            \n",
    "    return all_tuples\n",
    "\n",
    "# The directory path relative to the notebook\n",
    "directory_path = './qatuples_raw'\n",
    "\n",
    "# Call the function to read JSON files\n",
    "vetted_qa_tuples = read_json_files(directory_path)\n",
    "vetted_qa_tuples = [item for sublist in vetted_qa_tuples if sublist for item in sublist]\n",
    "vetted_qa_tuples[:3]  # Display the first 3 sublists for brevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of single-turn long-form problem training examples from the question-answer tuples. \n",
    "# plan: \n",
    "\n",
    "# Directory for single turn conversations\n",
    "single_turn_convs_dir = './single_turn_convs'\n",
    "if not os.path.exists(single_turn_convs_dir):\n",
    "    os.makedirs(single_turn_convs_dir)\n",
    "\n",
    "# Generation of single-turn long-form problem training examples from the question-answer tuples.\n",
    "single_turn_convs = []\n",
    "for idx, qatuple in enumerate(vetted_qa_tuples): \n",
    "    try:\n",
    "        conv = make_single_turn_conversation(qatuple)\n",
    "        final_conv = ensure_answer_is_same(qatuple, conv, logic_llm)\n",
    "        # Write to file if the conversation is not None\n",
    "        if final_conv is not None:\n",
    "            file_path = os.path.join(single_turn_convs_dir, f'conv_{idx}.json')\n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(final_conv, file, indent=4)\n",
    "\n",
    "        single_turn_convs.append(final_conv)\n",
    "    except Exception as e: # wow such error handling\n",
    "        print(\"!!!!--!!!! Error! \", e)\n",
    "    \n",
    "print(\"-------------- EXAMPLES CREATED ------------- STATS SO FAR:\")\n",
    "nones_singleturn = list(filter(lambda x: x is None, single_turn_convs))\n",
    "print(f\"Nones: {len(nones_singleturn)}\")\n",
    "print(f\"Non-nones: {len(single_turn_convs) - len(nones_singleturn)}\")\n",
    "print(f\"Total: {len(single_turn_convs)}\")\n",
    "# filter out nones\n",
    "single_turn_convs_filtered = list(filter(lambda x: x is not None, single_turn_convs))\n",
    "print(\"---------------- ONTO MULTITURN EXAMPLES GENERATION---------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper to group tuples for multiturn example generation (by text) and then run that helper\n",
    "def group_by_text(tuples_list):\n",
    "    # Dictionary to hold the groups with text as the key\n",
    "    groups = {}\n",
    "    \n",
    "    # Iterate over each tuple in the list\n",
    "    for question, answer, text, textname in tuples_list:\n",
    "        # If the text is not yet a key in the dictionary, add it with an empty list\n",
    "        if text not in groups:\n",
    "            groups[text] = []\n",
    "        \n",
    "        # Append the current tuple to the appropriate list\n",
    "        groups[text].append((question, answer, text,textname))\n",
    "    \n",
    "    # Return the values of the dictionary, which are the lists of tuples grouped by text\n",
    "    return list(groups.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_tuples_by_paragraph = group_by_text(vetted_qa_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiturn_helpers\n",
    "# These will probably be used for multiturn rapid-fire answering.\n",
    "\n",
    "# Idea: use multiple short answers to train the task of answering multiple questions in one response. Two-three short answers per response should be enough.\n",
    "def make_multiturn_character(qa_tuples):\n",
    "    plan, instructions = create_character_card_plan_many_tuples(qa_tuples,logic_llm) # I will reuse the many tuples function for short question-answers, there's a lot of prompting in here already\n",
    "    char = create_character_card_many_tuples(qa_tuples,plan,instructions,logic_llm) # creates a character card\n",
    "    return char, instructions\n",
    "\n",
    "def make_multiturn_scenario(qa_tuples,character):\n",
    "    plan = create_scenario_plan_many_tuples(qa_tuples,character,logic_llm)\n",
    "    scenario = create_scenario_many_tuples(qa_tuples,character,plan,logic_llm) # creates a scenario based on a character card and question/answer tuple\n",
    "    return scenario, plan\n",
    "\n",
    "def make_multiturn_conversation(qa_tuples,logic_llm):\n",
    "    # thought_plan = create_thought_plan_many_tuples(qa_tuples,character,scenario,logic_llm) # There IS a way to make multiturn chain of thought answering work: generate each pair of messages using a separate prompt or a separate function, each of which has only the thought plan for that question/answer pair. But simply cramming in all the step-by-step things will confuse the hell out of the poor model. So for the pre-alpha, we're skipping it and just giving the response, with no reasoning, in the multiturn convs.\n",
    "    character, instructions = make_multiturn_character(qa_tuples)\n",
    "    scenario, scenario_plan = make_multiturn_scenario(qa_tuples, character)\n",
    "    conv = multi_turn_conversation(qa_tuples, character, scenario, scenario_plan, logic_llm)\n",
    "    \n",
    "    return conv, \n",
    "\n",
    "def ensure_multiple_answers_are_same(multiple,conv,scenario,llm):\n",
    "    \"\"\"A loop, like the check accuracy ones for the multiple generation, that ensures that the answer is the same in the conversation as it is in the tuple.\"\"\"\n",
    "    retries = 0\n",
    "    c = conv\n",
    "    while retries <= 4:\n",
    "        consistence_checks = 0\n",
    "        while consistence_checks < DOUBLE_CHECK_COUNTER:\n",
    "            judgement = check_multiple_answers_consistent(multiple,c[0], llm)\n",
    "            if not judgement[0]:  # if not relevant\n",
    "                break\n",
    "            consistence_checks += 1\n",
    "        \n",
    "        if consistence_checks == DOUBLE_CHECK_COUNTER:  # if all question checks passed\n",
    "            return c\n",
    "        \n",
    "        # If we're here, question relevance check failed\n",
    "        retry = make_multiturn_conversation(multiple,logic_llm)\n",
    "        if retry is not None: # Note: currently retry CANNOT actually be BE None\n",
    "            c = retry\n",
    "        else:\n",
    "            # if we failed to generate a retry after it got broken the first time, this is just f*cked and it's a bad idea to waste compute on it\n",
    "            return None\n",
    "        retries += 1\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "REARRANGEMENTS_TO_TAKE = 3 # How many of the possible permutations of tuples in a group to take and make multiturn convs out of. Adjust higher to get more data out of less text, but it might be a bit repetitive. And it also might make your eval loss worthless if you aren't careful with how you shuffle your dataset when you're about to train.\n",
    "\n",
    "single_turn_convs_dir = './multi_turn_convs'\n",
    "if not os.path.exists(single_turn_convs_dir):\n",
    "    os.makedirs(single_turn_convs_dir)\n",
    "\n",
    "multi_turn_convs = []\n",
    "for group in qa_tuples_by_paragraph:\n",
    "    try:\n",
    "        # Get all possible permutations of tuples in the group\n",
    "        all_permutations = list(itertools.permutations(group))\n",
    "        \n",
    "        # Sample 8 permutations at random, or take all if there are less than 8\n",
    "        sample_size = min(REARRANGEMENTS_TO_TAKE, len(all_permutations))\n",
    "        sampled_permutations = random.sample(all_permutations, sample_size)\n",
    "        \n",
    "        # This will hold all conversations for the current group\n",
    "        group_convs = []\n",
    "        \n",
    "        # Iterate over each sampled permutation to create a conversation\n",
    "        for perm in sampled_permutations:\n",
    "            conv = make_multiturn_conversation(perm, logic_llm)\n",
    "            final_conv = ensure_group_answers_are_same(perm, conv, logic_llm)\n",
    "            \n",
    "            if final_conv is not None:\n",
    "                file_path = os.path.join(single_turn_convs_dir, f'conv_{idx}.json')\n",
    "                with open(file_path, 'w') as file:\n",
    "                    json.dump(final_conv, file, indent=4)\n",
    "            \n",
    "            group_convs.append(final_conv)\n",
    "    except Exception as e: # wow such error handling\n",
    "        print(\"!!!!--!!!! Error! \", e)\n",
    "    \n",
    "    # Add the list of conversations for this group to the main list\n",
    "    multi_turn_convs.append(group_convs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### REMEMBER NOTE: for both this, and the previous conversation generation, the user should A) have a randomized name and personality, and B) have a role, probably the most active role, in driving the plot forward (this all gets specified in the scenario and conv planning functions, and maybe the generation functions too). This will help the model respond better to a user with initiative. This is kinda bleeding into the future plans, but I don't see how I'll get good data without doing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mega cell of doom: combine all the tuple lists into one big list, with a column indicating what type of completion each example is (and whether it's a duplicate or not) and save it to a file. Do not shuffle, we want the order intact.\n",
    "# TODO\n",
    "# There's no way in hell what this needs to be won't change significantly between now and when it's actually ready to go in, so I'm not even going to write anything here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
